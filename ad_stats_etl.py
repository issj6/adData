#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šæ•°æ®ETLè„šæœ¬ï¼ˆæœ€å°å·¥ç¨‹ç‰ˆæœ¬ï¼‰
æŒ‰track_timeå½’å› ï¼Œæ”¯æŒæ»šåŠ¨é‡ç®—çª—å£å¤„ç†è¿Ÿåˆ°å›žè°ƒ
"""

import pymysql
from datetime import datetime, timedelta
import argparse
import logging
import sys

from db_config import SOURCE_DB_CONFIG, TARGET_DB_CONFIG, SOURCE_TABLE_NAME

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ad_stats_etl.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def connect_database(config):
    """è¿žæŽ¥æ•°æ®åº“"""
    return pymysql.connect(**config, cursorclass=pymysql.cursors.DictCursor, ssl_disabled=True)

def process_daily_aggregation(target_date: str, rollback_days: int = 7):
    """
    å¤„ç†æ—¥çº§èšåˆ
    
    Args:
        target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)
        rollback_days: å›žæ»šé‡ç®—å¤©æ•°ï¼Œå¤„ç†è¿Ÿåˆ°å›žè°ƒ
    """
    
    logger.info(f"ðŸ”„ å¼€å§‹å¤„ç†æ—¥çº§èšåˆ: {target_date} (å›žæ»š{rollback_days}å¤©)")
    
    # è®¡ç®—éœ€è¦é‡ç®—çš„æ—¥æœŸèŒƒå›´
    target_dt = datetime.strptime(target_date, '%Y-%m-%d')
    start_date = (target_dt - timedelta(days=rollback_days-1)).strftime('%Y-%m-%d')
    end_date = target_date
    
    logger.info(f"ðŸ“… é‡ç®—æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")
    
    source_conn = connect_database(SOURCE_DB_CONFIG)
    target_conn = connect_database(TARGET_DB_CONFIG)
    
    try:
        # åˆ é™¤é‡ç®—çª—å£å†…çš„æ—§æ•°æ®
        target_cursor = target_conn.cursor()
        delete_sql = """
            DELETE FROM ad_stats_daily 
            WHERE date_day >= %s AND date_day <= %s
        """
        target_cursor.execute(delete_sql, (start_date, end_date))
        deleted_rows = target_cursor.rowcount
        logger.info(f"ðŸ—‘ï¸ åˆ é™¤æ—§æ•°æ®: {deleted_rows} è¡Œ")
        
        # ä»Žæºè¡¨èšåˆæ•°æ®ï¼ˆæŒ‰track_timeå½’å› ï¼‰
        source_cursor = source_conn.cursor()
        
        aggregation_sql = f"""
            SELECT
                DATE(track_time) as date_day,
                up_id,
                ds_id,
                ad_id,
                channel_id,
                os,
                is_callback_sent,
                callback_event_type,
                COUNT(*) as request_count,
                SUM(CASE WHEN is_callback_sent = 1 THEN 1 ELSE 0 END) as callback_count
            FROM {SOURCE_TABLE_NAME}
            WHERE DATE(track_time) >= %s AND DATE(track_time) <= %s
            GROUP BY
                DATE(track_time),
                up_id,
                ds_id,
                ad_id,
                channel_id,
                os,
                is_callback_sent,
                callback_event_type
            ORDER BY date_day, ds_id, ad_id, is_callback_sent
        """
        
        logger.info("ðŸ“Š å¼€å§‹ä»Žæºè¡¨èšåˆæ•°æ®...")
        source_cursor.execute(aggregation_sql, (start_date, end_date))
        aggregated_data = source_cursor.fetchall()
        
        if not aggregated_data:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°éœ€è¦èšåˆçš„æ•°æ®")
            return True
        
        logger.info(f"ðŸ“Š èšåˆå®Œæˆ: {len(aggregated_data)} æ¡è®°å½•")
        
        # æ‰¹é‡æ’å…¥èšåˆæ•°æ®
        insert_sql = """
            INSERT INTO ad_stats_daily 
            (date_day, up_id, ds_id, ad_id, channel_id, os, is_callback_sent, callback_event_type, request_count, callback_count)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        batch_data = []
        for row in aggregated_data:
            batch_data.append((
                row['date_day'],
                row['up_id'],
                row['ds_id'],
                row['ad_id'],
                row['channel_id'],
                row['os'],
                row['is_callback_sent'],
                row['callback_event_type'],
                row['request_count'],
                row['callback_count']
            ))
        
        # åˆ†æ‰¹æ’å…¥
        batch_size = 1000
        inserted_count = 0
        
        for i in range(0, len(batch_data), batch_size):
            batch = batch_data[i:i + batch_size]
            target_cursor.executemany(insert_sql, batch)
            inserted_count += len(batch)
            
            if i % (batch_size * 10) == 0:
                target_conn.commit()
                logger.info(f"ðŸ“¥ å·²æ’å…¥ {inserted_count} / {len(batch_data)} æ¡è®°å½•")
        
        target_conn.commit()
        logger.info(f"âœ… èšåˆæ•°æ®æ’å…¥å®Œæˆ: {inserted_count} æ¡è®°å½•")
        
        # æ•°æ®éªŒè¯
        target_cursor.execute("""
            SELECT 
                COUNT(*) as total_rows,
                SUM(request_count) as total_requests,
                SUM(callback_count) as total_callbacks,
                MIN(date_day) as min_date,
                MAX(date_day) as max_date
            FROM ad_stats_daily 
            WHERE date_day >= %s AND date_day <= %s
        """, (start_date, end_date))
        
        validation = target_cursor.fetchone()
        logger.info(f"ðŸ“Š æ•°æ®éªŒè¯ - æ€»è¡Œæ•°: {validation['total_rows']}, "
                   f"æ€»è¯·æ±‚: {validation['total_requests']}, "
                   f"æ€»å›žè°ƒ: {validation['total_callbacks']}, "
                   f"æ—¥æœŸèŒƒå›´: {validation['min_date']} ~ {validation['max_date']}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ETLå¤„ç†å¤±è´¥: {e}")
        target_conn.rollback()
        return False
        
    finally:
        if 'source_cursor' in locals():
            source_cursor.close()
        if 'target_cursor' in locals():
            target_cursor.close()
        source_conn.close()
        target_conn.close()
        logger.info("ðŸ” æ•°æ®åº“è¿žæŽ¥å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¹¿å‘Šæ•°æ®ETLå¤„ç†ï¼ˆæœ€å°å·¥ç¨‹ç‰ˆæœ¬ï¼‰')
    parser.add_argument('--date', type=str, help='ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºæ˜¨å¤©')
    parser.add_argument('--rollback-days', type=int, default=7, 
                       help='å›žæ»šé‡ç®—å¤©æ•°ï¼Œç”¨äºŽå¤„ç†è¿Ÿåˆ°å›žè°ƒ (é»˜è®¤: 7å¤©)')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼ˆä¸æ‰§è¡Œå®žé™…ETLï¼‰')
    
    args = parser.parse_args()
    
    # ç¡®å®šç›®æ ‡æ—¥æœŸ
    if args.date:
        target_date = args.date
        # éªŒè¯æ—¥æœŸæ ¼å¼
        try:
            datetime.strptime(target_date, '%Y-%m-%d')
        except ValueError:
            logger.error("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
            sys.exit(1)
    else:
        # é»˜è®¤å¤„ç†æ˜¨å¤©çš„æ•°æ®
        target_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    logger.info(f"ðŸš€ å¯åŠ¨å¹¿å‘Šæ•°æ®ETLä»»åŠ¡")
    logger.info(f"ðŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")
    logger.info(f"ðŸ”„ å›žæ»šå¤©æ•°: {args.rollback_days}")
    
    if args.test:
        logger.info("ðŸ§ª æµ‹è¯•æ¨¡å¼ï¼Œè·³è¿‡å®žé™…ETLæ‰§è¡Œ")
        logger.info("âœ… æµ‹è¯•å®Œæˆ")
        return
    
    # æ‰§è¡ŒETL
    success = process_daily_aggregation(target_date, args.rollback_days)
    
    if success:
        logger.info("ðŸŽ‰ ETLä»»åŠ¡æ‰§è¡ŒæˆåŠŸ!")
        sys.exit(0)
    else:
        logger.error("ðŸ’¥ ETLä»»åŠ¡æ‰§è¡Œå¤±è´¥!")
        sys.exit(1)

if __name__ == "__main__":
    main()
